[id="fri"]

= The FRI Protocol

We now combine the two above ideas (testing two polynomials with half the queries, and splitting a polynomial into two smaller ones) into a protocol that only uses O(log d) queries to test that a function f has (more precisely, is close to a function of) degree less than d. This protocol is known as FRI (which stands for Fast Reed -- Solomon Interactive Oracle Proof of Proximity), and the interested reader can read more about it https://eccc.weizmann.ac.il/report/2017/134/[here]. For simplicity, below we assume that d is a power of 2. The protocol consists of two phases: a commit phase and a query phase.

== Commit phase
The prover splits the original polynomial fâ‚€(x)=f(x) into two polynomials of degree less than d/2, gâ‚€(x) and hâ‚€(x), satisfying fâ‚€(x)=gâ‚€(xÂ²)+xhâ‚€(xÂ²). The verifier samples a random value ğ›¼â‚€, sends it to the prover, and asks the prover to commit to the polynomial fâ‚(x)=gâ‚€(x) + ğ›¼â‚€hâ‚€(x). Note that fâ‚(x) is of degree less than d/2.

We can continue recursively by splitting fâ‚(x) into gâ‚(x) and hâ‚(x), then choosing a value ğ›¼â‚, constructing fâ‚‚(x) and so on. Each time, the degree of the polynomial is halved. Hence, after log(d) steps we are left with a constant polynomial, and the prover can simply send the constant value to the verifier.

A note about the domains: for the above protocol to work, we need the property that for every z in the domain L, it holds that -z is also in L. Moreover, the commitment on fâ‚(x) will not be over L but over LÂ²={xÂ²: x âˆŠ L}. Since we iteratively apply the FRI step, LÂ² will also have to satisfy the {z, -z} property, and so on. These natural algebraic requirements are easily satisfied via natural choices of domains L (say, a multiplicative subgroup whose size is a power of 2), and in fact coincide with those that we anyways need in order to benefit from efficient FFT algorithms (which are used elsewhere in STARK, e.g., to encode execution traces).

== Query phase
We now have to check that the prover did not cheat. The verifier samples a random z in L and queries fâ‚€(z) and fâ‚€(-z). These two values suffice to determine the values of gâ‚€(zÂ²) and hâ‚€(zÂ²), as can be seen by the following two linear equations in the two "`variables`" gâ‚€(zÂ²) and hâ‚€(zÂ²):

image::query1.png[query1]

The verifier can solve this system of equations and deduce the values of gâ‚€(zÂ²) and hâ‚€(zÂ²). It follows that it can compute the value of fâ‚(zÂ²) which is a linear combination of the two. Now the verifier queries fâ‚(zÂ²) and makes sure that it is equal to the value computed above. This serves as an indication that the commitment to fâ‚(x), which was sent by the prover in the commit phase, is indeed the correct one. The verifier may continue, by querying fâ‚(-zÂ²) (recall that (-zÂ²)âˆŠ LÂ² and that the commitment on fâ‚(x) was given on LÂ²) and deduce from it fâ‚‚(zâ´).

The verifier continues in this way until it uses all these queries to finally deduce the value of f_{log d}(z) (denoting f with a subscript log d, that we can't write due to Medium's lack of support for fully fledged mathematical notation). But, recall that f_{log d}(z) is a constant polynomial whose constant value was sent by the prover in the commit phase, prior to choosing z. The verifier should check that the value sent by the prover is indeed equal to the value that the verifier computed from the queries to the previous functions.

Overall, the number of queries is only logarithmic in the degree bound d.

Optional: To get a feeling why the prover cannot cheat, consider the toy problem where fâ‚€ is zero on 90% of the pairs of the form {z,-z}, i.e., fâ‚€(z) = fâ‚€(-z) = 0 (call these the "`good`" pairs), and non-zero on the remaining 10% (the "`bad`" pairs). With probability 10% the randomly selected z falls in a bad pair. Note that only one ğ›¼ will lead to fâ‚(zÂ²)=0, and the rest will lead to fâ‚(zÂ²)â‰ 0. If the prover cheats on the value of fâ‚(zÂ²), it will be caught, so we assume otherwise. Thus, with a high probability (fâ‚(zÂ²), fâ‚(-zÂ²)) will also be a bad pair in the next layer (the value of fâ‚(-zÂ²) is not important as fâ‚(zÂ²)â‰ 0). This continues until the last layer where the value will be non-zero with high probability.

On the other hand, since we started with a function with 90% zeros, it is unlikely that the prover will be able to get close to a low degree polynomial other than the zero polynomial (we will not prove this fact here). In particular, this implies that the prover must send 0 as the value of the last layer. But then, the verifier has a probability of roughly 10% to catch the prover. This was only an informal argument, the interested reader may find a rigorous proof https://eccc.weizmann.ac.il/report/2017/134/[here].

In the test described so far (and the above analysis) the probability that the verifier catches a malicious prover is only 10%. In other words the error probability is 90%. This can be exponentially improved by repeating the above query phase for a few independently sampled z's. For example, by choosing 850 z's, we get an error probability of 2{caret}{-128} which is practically zero.

In summary, the direct solution (test) requires too many queries to achieve the succinctness required by STARK. To attain logarithmic query complexity, we use an interactive protocol called FRI, in which the prover adds more information in order to convince the verifier that the function is indeed of low degree. Crucially, FRI enables the verifier to solve the low-degree testing problem with a number of queries (and rounds of interaction) that is logarithmic in the prescribed degree.

== Contributing

ğŸ¯ \{pp}+_\{pp}+STARKer: \{pp}+_\{pp}+ This book is an open source effort, made possible only by contributions from readers like you. If you are interested in making this resource better for other users - please suggest a change by following the instructions https://github.com/starknet-edu/starknetbook/blob/main/CONTRIBUTING.adoc[here]. ğŸ¯ ğŸ¯
